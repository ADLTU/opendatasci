Using Logistic Regression to predict Yes/No outcomes
Often we have to resolve questions with binary or Yes/No outcomes.
For example
Does a patient have cancer?
Will a team win the next game?
Will the customer buy my product?
Will I get the loan?
Want to use our data which has Yes/No outcomes for past events to make a prediction.

A familiar example

We are going to start by plotting something we understand in the real world, although we may never actually have plotted it before.
Let's say on the X axis is the number of goals scored by an NFL team over a season and say the outcome on the Y axis is whether they lost or won the game indicated by a value of 0 or 1 respectively.
Then a plot for these scores might look like this.
[[ plot of bivalued points Score vs Win/Loss ]]
So how do we predict whether we have a win or a loss if we are given a score?
Clearly linear regression is not a good model
Take a look at this.
[[ binary outcomes w linear plot ]]
We need a better way to model our data.


Motivating threshold functions

We are going to do this in two steps.
First we will just pull a function out of the data science bag of tricks and show that it works reasonably well.

And then we are going to understand how we came up with that function and how it is related to binary outcomes and odds.
But before that let's understand this a bit better

This function will need to have a value of zero for the loss scores and 1 for the win scores.
p(X) = 1/(1 - exp(b0 + b1X))
So we have our familiar linear function but we are going to plug it into a shape that has a threshold behavior.
[[ we need some more comments here to soothe the neuronal anguish the student is feeling right now and ease the transition to tacit acceptance ]]
So how does this predict yes/no outcomes and why this function of all functions?
First the how.
From the shape we can see that that if the score was ... or P(X) would predict an almost sure loss, if score was ... an almost sure win. But in the middle things would be somewhat fuzzy - we would have say even odds when the score was around ....
This actually allows us to a) use smooth functions which are easier to manage mathematically and b) handle the cases where we have a "gray" area in the middle like real world odds calculations do.
So notice we started talking about odds and that provides us a bridge to an explanation of how we cam up with the logistic function.
Odds mathematically speaking
We are going to take the notion of odds, put a simple mathematical framework around it and then create a way to use our previous knowledge of linear regression to create a model that predicts binary outcomes.
We need two bits of prior knowledge to understand this
a) Exponentials and Natural logarithms (Here's a set of review links) [[ ]]
b) A very elementary understanding of probability (p) Basically all we need to know is that it's a number between 0 and 1 and indicates the likelihood of an event occurring. We remind ourselves that p = 0 is as good as the event being impossible and p = 1 is as good as it being certain. 
We should also remind ourselves that if the probability of an event happening is P then the probability of it not happening is (1-p). 
That's all the probability we need.
As we progress along the journey we will have math gradually added and most of it will be at the simplest possible level needed to work with these techniques. We have a philosophy of "Just Enough Math" so if you see a math symbol in a lesson it's because we really need it.
Having said that let's start talking about odds.
When bettors say the odds of winning are 1:4 what is this in terms of probability? It means 1 part chance of winning to 4 parts chance of losing. Note that total parts = 5 and odds of winning is 1 out of 5. So p is 1/5 = 0.2 and 1-p is 0.8. The odds might be 1:1 which is called even odds and means p = 1/2 and 1-p = 1/2 i.e. equal chances of an even happening or not.
The odds might be 3:2 which means p=0.6 and 1-p = 0.4.
So depending on the ratio of p to 1-p we have more or less confidence in a bet winning.
So we talk about Odds Ratio = ( Prob of event happening )/( Prob of event not happening )
If OddsRatio is high say > 0.75 then even is very likely if < 0.25 very unlikely.
Mathematicians like to work with a function derived from this called the Logit function
logit = log(p/1-p) or LogOdds function
And here is where we wave a magic wand again and bring in our linear regression formula.
Let's say we want to plot a set of events on the X axis and the logit value for that even on the Y axis and we want to fit a linear model to this.
So we would say
logit(p) = log(p/1-p) = b0 + b1X where X is the "value" of the event.
So here instead of Y = b0 + b1X we want to plot logit(p) on the Y axis and the event or the score on the X axis.
So this is how the linear model slips in - we want to express log odds as a linear function of score.
Patience now we are just one step away.
We don't really like plotting LogOdds as it involves awkward intermediate steps of calculating the logit() value.
So we do the hard work once and for all and solve for p as a function of X
and when we do that we get
p(X) = 1/(1-exp(b0 + b1X))
which, voila, is what we pulled out of the hat - but now we know why we pulled that particular one and why we didnt use any other.
[[mention tanh x ?? or other threshold functions ??]]


==== 
separate notebook

Ideally we would like a step function like this
[[ plot of step function ]]
which stays 0 below a threshold and then goes to 1 after the threshold.
But this is problematic for two main reasons.
a) Mathematically such discontinuous functions are very hard to work with and need advanced math to wrangle formally. b) Our real world data rarely splits so easily one a single threshold value so a step function will not even work very well most of the time.
Instead we want to have a function that is 0 on lower values of score, 1 on higher values of score and go smoothly from 0 to 1. So here's a function that does this (imagine a whooshing sound as we pull it out of a hat).
Logistic Function
p(z) = 1/(1-exp(z))
[[ plot of the function ]]
But what is z?
Well z is actually a linear function of the form b0 + b1X.
So as a function of X we have

====


david schachter python
lmgtfy.com


====


