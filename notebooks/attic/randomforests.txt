### Improving our results by consulting a group of experts
Often a problem that needs to be tackled is so large or complex that we need a group of experrts not just a single one to tackle it.  Heck, Linux is such a complex system that building it took hundreds of experts.
What if we could harness the decision making power and the subject matter expertise of many experts?  In Data Science there is a technique called Random Forests which is akin to this.  In this approach, each expert uses, you guessed it, a tree-based algorithm to do theri bit and then a collection of such trees is used to compute or evolve a model that is better than the output of any one expert.

### Decision trees from data.
Consider a pool of college applicants applying to SuperExclusiveU.  The average SAT score for admission has, historically been 2200.  And the average GPA 4.9.  We are given the application info on say a 1000 applicants.  We are asked to create a model that will allow us to predict students most likely to be admitted.  How do we go about doing this?

One approach would be to first divide the applicants into those that have SAT score > 2200 and then call this the "more likely"
group.  Then to further test for the GPA in this group and split it into two based on GPA > 4.9 vs GPA < 4.9.  we call the former subgroup, most likely and the latter a high maybe.

Then we do the same thing to the group with SAT score < 2200 calling the high GPA subgroup a maybe, and the low GPA subgroup a probably not.  This seems reasonable but there are a number of questions that arise.

* What if we split on GPA first and then SAT scores - would we get the same groupings?  i.e. what is the best way to split?
* What if we used more criteria such as essay evaluation scores, extra curriculars, awards and distinctions say in sports etc. i.e. how many attributes should we use to create splits
* We were given averages, but what about the spread, what about outliers? i.e. how does the distribution of attributes affect misclassification?

The science of decision trees quantifies all this using measures called Information Gain and Entropy.  Essentially we want to have just the right amount of splits so that we don't keep splitting a group once we have the "best" split.  So how do we know when one way to split is better than another.  Obviously if one split leads to a clean partition into admit vs reject then it's good.  But what if we split on say essay scores right at the top.  We might get groups that have wide variation in GPA, STA in both halves. So we really haven't improved our ability to predict much because  both groups seem equally mixed.

This kind of variation in a set indicates a higher "entropy" while a set with all identical members has very low or zero "entropy" so when we split a set we want the halves to be more distinct from each other and the members in the group to be more like each other - ie we want entropy to go down as we keep splitting.  So if we use an approach that doesn't reduce the entropy by much it is probably not a good attribute or a good value to split on.  

If we take a decision tree that has been created and we reverse the process, then when we combine two nodes, we will increase entropy or variation as groups get combined,  The gain in entropy is called Information Gain.  SO the best splits are those which give the best Infromation Gain when reversed.

This is all very loose but has a strong mathematical foundation that is used to construct the  modeling software that creates such "decision tree" models.

When given a set of samples with many attributes, a decision tree model will identify the attributes that are best to split on and the values of those attributes that we should use to do the splitting.  It will then print out a number of parameters,
including number of attributes used to split, which ones, and Information Gain,... etc.

So how does this software decide the best tree?  It tries every one and compares Information Gain for each and then picks the best one.


### Now for the Forest
So if we consider our "expert" to have in their head a decision tree modeler and we assemble say 100 such experts, then loosely this is how Random Forests are constructed.  We want a collection of experts to decide our result, expecting that the result will be much better than a single one.  So we will need some way to decide how to collate and sort through the "opinions".  

If you recall the Olympic Gymnastic sompetitions or diving competitions where a panel of judges scores a participant, you might remember that the top and bottom scores are dropped and the rest are averaged.  A Random Forest algorithm uses such techniques to eliminate soem of the opinions but might randomly drop some percentage and then rerun the "competition", doing this each time and then averaging the result after say 100 such trials.

### Why use such complicated techniques?
Well for one, they are more accurate, as mathematicaly provable.  But also because, when we have 10's or 100's of attributes Random Forests are able to surface the most significant ones and use those in their modeling without any extra effort on pour part.  So what's the catch?  This comes at some computational cost so our model may run for many minutes instead of a few seconds even with a few thousand samples, since orders of magnitude more calculations are being done.  However there are many more benefits for this one cost.  
Random Forests are much more tolerant of missing values, of bad data, ouliers and can handle mixed data types, numerical and categorical.

We will explore a rich data set generated from the accelerometer and gyroscope of mobile phones, and use it to understand various activities of the user - such as sitting, standing walking etc, based on particular combinations of the data attributes.  Our data has more than 500 such attributes and the data is also messy and rich so this is a good candiadte for combining domain knowledge with the power of Random Forests in the exploration and analysis to follow.














