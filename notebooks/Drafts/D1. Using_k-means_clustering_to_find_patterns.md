How do I explore my data when I have no examples?
=================================================

## Using k-Means Clustering to find patterns 


### Introduction

Often we are given a large mass of data with no labels.  That is, the data does not tell us what a "good" data item is and what a "bad" one is. So we have no prior idea what to look for.  In cases like these,  where we want to bootstrap our data exploration, to find some hook, some insight to get started,  looking for similar items or "clusters" is a well known approach.

Clustering techniques operate on the attributes of our data and find "clumps" of data items that have attribute values that are close. Pne of the most common clustering techniques is "K-Means Clustering" which is simple to understand but powerful in impact.  It is also "computationally hard" from a theoretical point of view, i.e. as simple as it looks it can be demanding of computational resources and take time to converge to a solution.


### Supervised and unsupervised learning

K-Means Clustering is one of the many techniques that constitute "unsupervised learning".
So far we've looked at data that had some examples of good data and some of bad, or some examples of data that was classified in category A othe rin B and so on.  That is we could learn from examples.  This mode of learning is called "supervised learning" because you are given some prior training of how to proceed.   Most often one creates models that fit the training data and use these to predict over future or unknown data sets.

Quite often no such prior knowledge exists and we are just given the data.  This mode, where we have to find patterns in data without guidance of prior knowledge, is called ""unsupervised learning" because there is no "supervision" in the form of examples of good classification.

So we have to dive in and dig out *some* nugget(s) of wisdom, no matter what, to get started.
K-Means Clustering is our "power excavator" to clear the ""data underbrush" and clear the way for laying the foundation of our grand data analysis edifice.


### What is it and how does it work?

K-means Clustering is called so because it involves it operates by computing the "mean" of some attributes and that mean then becomes the center of the cluster. There are a small number K of such clusters. That is the technique consists of computing K number of means leading to clustering.

How do we choose this K?  If we have some idea of what we are looking for or how many clsuters we expect or want, then we set K to be that number before we start the engines and let the algorithm rev along.

If we don't know how many there are then our exploration will take a little longer and involve some trial and error as we try say K=3,4,5 until we see that the clusters are making some sense to us in our domain.  

The algorithm starts by choosing K points in the data as "cluster centers" at random just to get started. Then it ietrates in steps and at each step it decides which cluster to assign a point to based on which cluster center is closest.

Once that is done it we have a new arrangement of points and so the "center" or "mean" of the cluster" is computed again because it will have most probably shifted.  

When does it not shift?  When we have a stable clustering. That is when we have iterated till we get no benefit from iterating further then that is our result.


### A toy example