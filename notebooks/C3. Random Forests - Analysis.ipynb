{
 "metadata": {
  "name": "C3. Random Forests - Analysis"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forests - Analysis.\n",
      "===\n",
      "***\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Our goal for this phase is to use the reduced variable data set from our exploration phase to create a model predicting human activity, using Random Forests.\n",
      "\n",
      "To remind ourselves, the variables we will use are:\n",
      "\n",
      "* tAccMean, tAccSD tJerkMean, tJerkSD\n",
      "* tGyroMean, tGyroSD tGyroJerkMean, tGyroJerkSD\n",
      "* fAccMean, fAccSD, fJerkMean, fJerkSD,\n",
      "* fGyroMean, fGyroSD, fGyroJerkMean, fGyroJerkSD,\n",
      "* fGyroMeanFreq, fGyroJerkMeanFreq fAccMeanFreq, fJerkMeanFreq\n",
      "* fAccSkewness, fAccKurtosis, fJerkSkewness, fJerkKurtosis\n",
      "* fGyroSkewness, fGyroKurtosis fGyroJerkSkewness, fGyroJerkKurtosis\n",
      "* angleAccGravity, angleJerkGravity angleGyroGravity, angleGyroJerkGravity\n",
      "* angleXGravity, angleYGravity, angleZGravity\n",
      "* subject, activity  \n",
      "\n",
      "Of these,   \n",
      "\n",
      "* all except the last two are numeric.  \n",
      "* 'subject' is an integer identifying a person, one of 21 from 1 to 27 with some missing. \n",
      "* 'activity' is a categorical variable - one of six activities identified earlier -  \n",
      "* 'sitting', 'standing', 'lying', 'walking', 'walking up', 'walking down'.  \n",
      "\n",
      "Why do we use Random Forests? We are using Random Forests [4] in our model due to the relatively high accuracy of this method and the complexity of our data.\n",
      "\n",
      "These are two major reasons to bring out the heavy artillery of Random Forests, especially when we have too many attrubutes even in a simplified set of attributes. \n",
      "\n",
      "## Methods\n",
      "\n",
      "\n",
      "### Expository Segue on Experiment design \n",
      "\n",
      "\n",
      "Typically in analysing such data sets we are creating a model that uses the data we are given.  How do we know the model will work for other data?  The real answer is \"We don't\".  And there's no way we can be sure that we can create a model that will work for new data.  \n",
      "\n",
      "But what we **can** do is reduce the chances that we are creating an \"overfitted\" model. That is a technical term for a model that works wonderfully on the given data (i.e well fitted to it) and fails on the next data sample that comes along.  Since we want ot have a model that performs well on given and future unknown data, the idea is we should be willing to give up accuracy on the known data to get overall better accuracy.  If we optimize on the knwon data alone, we fall into an \"overfitting\" trap. There's a way to design our modeling experiment so we avoid that trap.  Here's how.  \n",
      "\n",
      "We take the full given data set and we keep some of the given data aside which we don't use for modeling at all.  This \"held out\" set of data is called the \"test set\".  The test set will not be used in creating the model and will be used as, well .., a test of how good our model is.  \n",
      "Next we take our remaining (non test) data set and we further divide it so that we have a larger set called the \"training set\" and a smaller set we call the \"validation set\".\n",
      "\n",
      "Then we create our model using the training set and look at how well it performs on the validation set.  Here we are using our validation set as a mini test set.\n",
      "We are allowed to tweak our modeling as much as we want using the training and validations sets but we are **not** allowed to look at the held out, test set until we are ready to declare we are done modeling.  Then we apply the model to our held out test data -- if the test data also show an acceptable error rate we have a good model.  \n",
      "\n",
      "Since we have no knowledge of the test set in our modeling process, the test set represents an example of unknown future samples that may come from the same source, i.e. from the same \"distribution\" to be formal.  \n",
      " \n",
      "However if we get a bad error rate from the test data we have a problem.  We cannot keep tweaking the model to get a better test result because then we are simply overfitting again.  So what do we do?  We are allowed to mix up all the data, hold out a new test set which has to be different at least in part from the old one, and then we repeat the exercise.  \n",
      "\n",
      "Often in competitions like the Kaggle competitions, the test set is never revealed.\n",
      "You submit a model, they (Kaggle) apply it to the test set and give you back a numerical measure of how well your model did. Then you try to improve your model if you can.  All along you never get to use the test set in your model - as it should be.  The famous \"Netflix Million Dollar Challenge\" was also run in this fashion.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Our experiment design\n",
      "\n",
      "We hold out the last 4 subjects in the data as a test set and the rest are used for our modeling.   Why do we do this?  The data set, if we look at the supporting docs, suggests that we use the last 4 as test subjects.  So in our first pass at this we might as well just follow the instructions.  All rows relating to these 4 will be held out and not used during modeling at all.\n",
      "\n",
      "Of the 17 remaining subjects we use the first 12 subjects as the training set and remaining 5 as the validation set.   Why this proportion? Typically 30% of the the training data is used as validation set and 70% used for actual training.  The validation set is used as our \"internal\" test set, not used in modeling and held out for each validation step.  The difference between the actual test set and the validation set is that we are allowed to keep tuning our model as long as we keep mixing up the data after each attempt and re-extraction of a validation set.\n",
      "\n",
      "There is a methodology that takes this step even further and does n-fold validation.  The training set is divided into n (n is often 10) equal parts and then each part is used as a validation set while the rest used for training, with n such modeling exercises being conducted.\n",
      "Then some averaging is done to create the best model.\n",
      "\n",
      "We do not do n-fold validation here.\n",
      "\n",
      "We divided our data based on the 'subject' variable as we included \u2018subject\u2019 in our model and want to keep all test data separate.  What does this mean?  The test data should actually be data about which we have no information at all - i.e. it needs to be independent of the training data.  So suppose we did not separate out the data on the 4 test individuals but we just decided that we would mix up all the rows and take say 20% as test data, chosen randomly.\n",
      "\n",
      "Note that we have some 7,000 plus rows so we have a few hundred rows on each individual.  So if we mixed it all up and chose randomly, then we would most probably get data from all 21 individuals in our test set.  And all 21 in our training set.  The test set would not be independent of the training set as both would have somewhat similar mixtures of data.\n",
      "Thus the held out set would not really provide a useful reality check - we have statistically similar info in our training set already i.e. the test set has leaked into the training set.\n",
      "\n",
      "This would be similar to the situation where we had a homework exercise which was later solved in class the next day. Then we received a quiz question set which had questions very similar to the homework with just some numbers changed.  It would not really test our understanding of the subject matter, only our ability to understand the homework (= overfitting).  Then when we got a different set of problems on the midterm, and we got a poor score that would represent a bad error rate on the held out set.\n",
      "\n",
      "So when we keep aside our test set, separated by all rows for certain individuals, we know that the training set has no leaked information about these individuals.  It is important to be very diligent about the test data in this fashion, so that we can have a degree of confidence that our model is not overfitting our sample data.\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training\n",
      "\n",
      "We now run our RandomForest modeling software on our training set, described earlier, and derive a model along with numeric measures of the quality of our model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We pull in the training, validation and test sets created according to the scheme described\n",
      "# in the data exploration lesson.\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "samtrain = pd.read_csv('../datasets/samsung/samtrain.csv')\n",
      "samval = pd.read_csv('../datasets/samsung/samval.csv')\n",
      "samtest = pd.read_csv('../datasets/samsung/samtest.csv')\n",
      "\n",
      "# We use the Python RandomForest package from the scikits.learn collection of algorithms. \n",
      "# The package is called sklearn.ensemble.RandomForestClassifier\n",
      "\n",
      "# For this we need to convert the target column ('activity') to integer values \n",
      "# because the Python RandomForest package requires that.  \n",
      "# In R it would have been a \"factor\" type and R would have used that for classification.\n",
      "\n",
      "# We map activity to an integer according to\n",
      "# laying = 1, sitting = 2, standing = 3, walk = 4, walkup = 5, walkdown = 6\n",
      "# We use our custom method called remap_col\n",
      "# Code is in supporting library randomforest.py\n",
      "\n",
      "import randomforests as rf\n",
      "samtrain = rf.remap_col(samtrain,'activity')\n",
      "samval = rf.remap_col(samval,'activity')\n",
      "samtest = rf.remap_col(samtest,'activity')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.ensemble as sk\n",
      "rfc = sk.RandomForestClassifier(n_estimators=500, compute_importances=True, oob_score=True)\n",
      "train_data = samtrain[samtrain.columns[1:-2]]\n",
      "train_truth = samtrain['activity']\n",
      "model = rfc.fit(train_data, train_truth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One measure of the quality of a model is an \"Out of Bag\" estimate.  This term comes from \"Bagging\" or \"Bootstrap Aggregating\" which is a way of generating random subsamples of the given data (\"Bootstrapping\") and using these samples as representatives of actual samples taken from the full population from which our data set was sampled.  The bootstrap subsample is then used to create a model.  Many (10 to 100 or more) such samples are taken and the models \"averaged\" out to give an \"aggregate\" model. Hence Bootstarp Aggregation.\n",
      "Each time a bootstrap subsample is taken some data points are left out, by definition.  These are called \"Out of Bag\". The \"Out of Bag\" or OOB set is like an unknown test set for that subsample and the model that is created from it. The Out of Bag set is used to estimate how good the model is for this particular bootstrap subsample.  Here also, we \"average\" out the Out of Bag error for each boostrap and get an overall score called Out of Bag score.  the closer this is to 1 the better.  \n",
      "A score of 0.95 means our error, averaged out, was 1 - 0.95 = 0.05 ie 5 per cent. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The OOB (Out of Bag) score for our model.\n",
      "rfc.oob_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.97870722433460078"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can do some more probing using the tools our modeling software gives us.  One kind of \"algorithm MRI\" so to speak, is the feature importance ranking - which looks inside our model black box and evaluates how important each input variable is, i.e. how much it contributes to our model.  We can rank the input variables, or \"features\" and, say, just pick the top 10 or even 5 for reasons of computational efficiency.  \n",
      "We should, of course, be willing to give up some accuracy since we are dropping some of the \"information\" in the model coming from the dropped features.  As in any real world endeavour, what is \"best\" is relative to the tradeoffs we are willing to make.\n",
      "We rank the top 10 features in or model, based on the score assigned by our modeling software."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use \"feature importance\" scores to see what the top 10 important features are\n",
      "fi = enumerate(rfc.feature_importances_)\n",
      "cols = samtrain.columns\n",
      "# Python's Pandas data frame adds a spurious 'Unnamed' column in 0 position hence starting at col 1\n",
      "([(value,cols[1:][i]) for (i,value) in fi if value > 0.038])\n",
      "# How did we come up with the number 0.038 ?\n",
      "# Well, we just played around with a threshold until we got the top 10 :-)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[(0.05116868066026755, 'tAccStd'),\n",
        " (0.041605308853046939, 'tJerkMean'),\n",
        " (0.043732886172058567, 'tJerkSD'),\n",
        " (0.046820767579868944, 'fAccMean'),\n",
        " (0.057266514167116485, 'fAccSD'),\n",
        " (0.044233167230432462, 'fJerkMeanFreq'),\n",
        " (0.038166591774660849, 'fGyroMean'),\n",
        " (0.12896107947696558, 'angleXGravity'),\n",
        " (0.18069876691128506, 'angleYGravity'),\n",
        " (0.04386855188604262, 'angleZGravity')]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now use our model to predict activity given the inputs and see how well we do. Using our validation set and separately our test set we get the following results from our analysis of errors in the predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pandas data frame adds a spurious unknown column in 0 position hence starting at col 1\n",
      "# not using columns \"subject\" and \"activity\" \n",
      "# ie target is in last columns hence -2 == dropping last 2 cols\n",
      "\n",
      "val_data = samval[samval.columns[1:-2]]\n",
      "val_truth = samval['activity']\n",
      "val_pred = rfc.predict(val_data)\n",
      "\n",
      "test_data = samtest[samtest.columns[1:-2]]\n",
      "test_truth = samtest['activity']\n",
      "test_pred = rfc.predict(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Prediction Errors and Computed Error Measures  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"mean accuracy score for validation set = %f\" %(rfc.score(val_data, val_truth)))\n",
      "print(\"mean accuracy score for test set = %f\" %(rfc.score(test_data, test_truth)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mean accuracy score for validation set = 0.829127\n",
        "mean accuracy score for test set = 0.896970"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use the confusion matrix to see how observations were misclassified as other activities\n",
      "# See [5]\n",
      "import sklearn.metrics as skm\n",
      "test_cm = skm.confusion_matrix(test_truth,test_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize the confusion matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "pl.matshow(test_cm)\n",
      "pl.title('Confusion matrix for test data')\n",
      "pl.colorbar()\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAADzCAYAAAC13+t7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n"
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute a number of other common measures of prediction goodness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now compute some commonly used measures of prediction \"goodness\", viz.  \n",
      "Accuracy [6], Precision [7], Recall [8], F1 Score [9]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Accuracy\n",
      "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy = 0.896970\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Precision\n",
      "print(\"Precision = %f\" %(skm.precision_score(test_truth,test_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Precision = 0.899462\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Recall\n",
      "print(\"Recall = %f\" %(skm.recall_score(test_truth,test_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Recall = 0.896970\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# F1 Score\n",
      "print(\"F1 score = %f\" %(skm.f1_score(test_truth,test_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F1 score = 0.897293\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusions\n",
      "\n",
      "We can make the following concrete conclusions looking at the above results.\n",
      "\n",
      "* Random Forests give us satisfactory error rates and predictive power in this scenario.\n",
      "* \ufffcUsing domain knowledge it is possible to get surprisingly high values of predictive measures, and low error rates on validation and test sets.  This is supported by the results, i.e. ~90% on predictive measures, OOB error estimates ~2%.\n",
      "* Focusing on magnitude and angle information for acceleration and jerk in the time and frequency domains gives us a model with surprising predictive power.  It's possible that a brute force model will give better predictive power but it would simply show us how to blindly apply software.  If for some reason the model misbehaved or failed, we would not have any insight at all as to why.  Instead we used domain knowledge to focus on insight and in the process created a model with interpretive value.\n",
      "* Model performance on the test set is better than on the validation set as seen in the two \u201cTotal\u201d rows above and each individual activity.\n",
      "\n",
      "Let's see how we might be able to improve the model in future.  It's always good to note the possible ways in which our model(s) might be deficient or incomplete or unfinished so we don't get overconfident about our models and overpromise what they can do.\n",
      "\n",
      "### Critique\n",
      "\n",
      "* Our model eliminated a number of Magnitude related attributes such as -mad, -max, -min also a number of Gyro related variables during the variable selection process using domain knowledge. These may be important but this was not tested.  We may want to look at that the next time we do this.\n",
      "\n",
      "* Variable importance should be investigated in detail - i.e. we really ought to look at how we can use the smaller number of attributes identified as important, to create the model and see what the difference is. Computationally this would be more efficient. We could even use simpler methods like Logistic Regression to do the classification from that point on, using only the reduced set of variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "[1] Original dataset as R data https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda  \n",
      "[2] Human Activity Recognition Using Smartphones http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones  \n",
      "[3] Android Developer Reference http://developer.android.com/reference/android/hardware/Sensor.html    \n",
      "[4] Random Forests http://en.wikipedia.org/wiki/Random_forest    \n",
      "[5] Confusion matrix http://en.wikipedia.org/wiki/Confusion_matrix  \n",
      "[6] Mean Accuracy http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1054102&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1054102  \n",
      "[7] Precision http://en.wikipedia.org/wiki/Precision_and_recall  \n",
      "[8] Recall http://en.wikipedia.org/wiki/Precision_and_recall  \n",
      "[9] F Measure http://en.wikipedia.org/wiki/Precision_and_recall  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: \"Charis SIL\", Palatino, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 120%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }\n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "<IPython.core.display.HTML at 0x109d7c550>"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    }
   ],
   "metadata": {}
  }
 ]
}